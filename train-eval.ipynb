{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train-eval.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fgUBlclwDxc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "b504802e-8ac7-4cb8-c130-b7048df104a4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 10 04:04:11 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRWNJyRtDxc9",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex /apex\n",
        "!pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" /apex/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AbYDWNHqDxdD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "fe32911c-bbb1-4d9b-cdac-ad9c021b81b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls \"/content/gdrive/My Drive/lab/SSD/\"\n",
        "gdrive_dir = \"/content/gdrive/My Drive/lab/SSD/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "datasets  models  test.zip  train-eval.ipynb  trainval.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I0_DA3T4DxdK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1b1f3781-2393-4ea9-be45-4d28754abe0c"
      },
      "source": [
        "!unzip -q \"/content/gdrive/My Drive/lab/SSD/trainval.zip\" -d \"/content\"\n",
        "!git clone https://github.com/ncthuan/single-shot-detection-pytorch\n",
        "!cp single-shot-detection-pytorch/*.py .\n",
        "!pip install albumentations==0.4.6 -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'single-shot-detection-pytorch'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 71 (delta 41), reused 47 (delta 19), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (71/71), done.\n",
            "\u001b[K     |████████████████████████████████| 122kB 9.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 952kB 18.0MB/s \n",
            "\u001b[?25h  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FM286mawDxdQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4d0c52fb-18e3-42d8-ff9f-07249995ec3c"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.cuda as cuda\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from dataset import VOCDataset, collate_fn\n",
        "from model import SSD300, MultiBoxLoss\n",
        "from utils import create_data_lists, clip_gradient, save_checkpoint, calculate_mAP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q-ZAhMWoDxdV",
        "colab": {}
      },
      "source": [
        "from apex import amp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KqWYYcDRDxdb"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDai5wh2MSYc",
        "colab_type": "code",
        "colab": {},
        "outputId": "5ce870eb-31bd-470a-f6dd-3bf64191e669"
      },
      "source": [
        "from utils import create_data_lists\n",
        "voc07_path = os.path.join('datasets/', 'VOC2007/')\n",
        "voc12_path = os.path.join('datasets/', 'VOC2012/')\n",
        "create_data_lists(voc07_path, voc12_path, output_folder='datasets/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "There are 17202 training images. Files have been saved to output_folder: datasets/.\n",
            "\n",
            "There are 4301 validation images. Files have been saved to output_folder: datasets/.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73xuVqdoMSYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "943f81c3-9241-4e3b-d66a-694d38e1377e"
      },
      "source": [
        "!ls datasets/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label_map.json\t   TRAIN_objects.json  VAL_objects.json  VOC2012\n",
            "TRAIN_images.json  VAL_images.json     VOC2007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdh5PCNcMSYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "trainset = VOCDataset(data_folder='datasets/', json_files=('TRAIN_images.json', 'TRAIN_objects.json'), augment=True)\n",
        "valset = VOCDataset(data_folder='datasets/', json_files=('VAL_images.json', 'VAL_objects.json'))\n",
        "\n",
        "dataloaders = dict(\n",
        "    train = DataLoader(trainset, batch_size=16, collate_fn=collate_fn, shuffle=True, num_workers=2),\n",
        "    val = DataLoader(valset, batch_size=64, collate_fn=collate_fn, shuffle=False, num_workers=4),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XXqI4BFMSYs",
        "colab_type": "text"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTigT1YdMSYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_objects_stat(dataset):\n",
        "    targets = dataset.targets\n",
        "    objects = list()\n",
        "    for target in targets:\n",
        "        objects.extend(target['labels'])\n",
        "    print('Total number of annotated objects:', len(objects))\n",
        "    sns.countplot(objects)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO16PlwCMSYy",
        "colab_type": "code",
        "colab": {},
        "outputId": "540b381a-32b5-4a19-a2f7-677003b2190f"
      },
      "source": [
        "label_map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aeroplane': 1,\n",
              " 'bicycle': 2,\n",
              " 'bird': 3,\n",
              " 'boat': 4,\n",
              " 'bottle': 5,\n",
              " 'bus': 6,\n",
              " 'car': 7,\n",
              " 'cat': 8,\n",
              " 'chair': 9,\n",
              " 'cow': 10,\n",
              " 'diningtable': 11,\n",
              " 'dog': 12,\n",
              " 'horse': 13,\n",
              " 'motorbike': 14,\n",
              " 'person': 15,\n",
              " 'pottedplant': 16,\n",
              " 'sheep': 17,\n",
              " 'sofa': 18,\n",
              " 'train': 19,\n",
              " 'tvmonitor': 20,\n",
              " 'background': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smssMy8NMSY6",
        "colab_type": "code",
        "colab": {},
        "outputId": "06896d90-e038-4224-e6c9-d92306f8038d"
      },
      "source": [
        "visualize_objects_stat(trainset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of annotated objects: 49949\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6mdg_QuMSZA",
        "colab_type": "code",
        "colab": {},
        "outputId": "dea19af2-41c7-467f-b956-be492c17ab48"
      },
      "source": [
        "visualize_objects_stat(valset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of annotated objects: 12250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8WUVt5FvDxdo"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6wmuEoODxdq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "e1624253-861c-4211-ede2-bb1a7095981b"
      },
      "source": [
        "device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\") \n",
        "\n",
        "checkpoint_path = gdrive_dir+'models/checkpoint_ssd300.pt'\n",
        "checkpoint = None #torch.load('checkpoint_path')\n",
        "vgg16_dir = 'models/'\n",
        "\n",
        "# Initiate model instance\n",
        "MySSD300 = SSD300(n_classes=21, vgg16_dir=vgg16_dir, checkpoint=checkpoint).to(device)\n",
        "\n",
        "loss_func = MultiBoxLoss(priors_cxcy=MySSD300.get_prior_boxes(), threshold=0.5, neg_pos_ratio=3, alpha=1.)\n",
        "\n",
        "optimizer = torch.optim.Adam(MySSD300.parameters(), lr=1e-3, weight_decay=5e-4)\n",
        "\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.975, last_epoch=-1)\n",
        "\n",
        "grad_clip = None\n",
        "\n",
        "# Mixed precision\n",
        "#MySSD300, optimizer = amp.initialize(MySSD300, optimizer, opt_level='O1')\n",
        "\n",
        "if checkpoint:\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    exp_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.975, last_epoch=checkpoint['epoch'])\n",
        "    #amp.load_state_dict(checkpoint['amp'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3SGP1PteDxdv",
        "colab": {}
      },
      "source": [
        "def train_epoch(model, trainset_loader, loss_func, optimizer, epoch_id):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for step, (imgs, boxes, labels,_) in enumerate(trainset_loader):\n",
        "        # move input data to GPU\n",
        "        imgs = imgs.to(device)\n",
        "        boxes = [b.to(device) for b in boxes]\n",
        "        labels = [l.to(device) for l in labels]\n",
        "        \n",
        "        # forward\n",
        "        predicted_offsets, predicted_scores = model(imgs)\n",
        "        loss = loss_func(predicted_offsets, predicted_scores, boxes, labels)\n",
        "        \n",
        "        # backward & optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        #with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "        #    scaled_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch_id == 1 and step % 100 == 0:\n",
        "            print(f'Epoch 1 - step {step}: train_loss: {loss.item()/trainset_loader.batch_size:.4f}')\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        \n",
        "    return train_loss/len(trainset)\n",
        "\n",
        "def eval_epoch(model, valset_loader, loss_func):\n",
        "    '''\n",
        "    '''\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for step, (imgs, boxes, labels,_) in enumerate(valset_loader):\n",
        "            # move input data to GPU\n",
        "            imgs = imgs.to(device)\n",
        "            boxes = [b.to(device) for b in boxes]\n",
        "            labels = [l.to(device) for l in labels]\n",
        "            \n",
        "            predicted_offsets, predicted_scores = model(imgs)\n",
        "            loss = loss_func(predicted_offsets, predicted_scores, boxes, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    return val_loss/len(valset)\n",
        "\n",
        "\n",
        "def train_model(model, dataloaders, optimizer, exp_lr_scheduler, loss_func, epoch=1):\n",
        "    train_loss_hist, val_loss_hist = [], []\n",
        "    for i in range(1,epoch+1):\n",
        "        start_time = time.time()\n",
        "        train_loss = train_epoch(model, dataloaders['train'], loss_func, optimizer, i)\n",
        "        exp_lr_scheduler.step()\n",
        "        training_time = round(time.time() - start_time)\n",
        "        \n",
        "        # save_checkpoint(epoch, model, optimizer) #every 3 epoch\n",
        "        if i % 3 == 0:\n",
        "            start_time = time.time()\n",
        "            torch.save({'epoch': i, \n",
        "                        'model': model.state_dict(), \n",
        "                        'optimizer': optimizer.state_dict(), \n",
        "                        #'amp': amp.state_dict()\n",
        "                       }, checkpoint_path)\n",
        "            print(f'Epoch {i} - Model saved at checkpoint_path | Time consumed: {round(time.time()-start_time)}s')\n",
        "        \n",
        "        start_time = time.time()\n",
        "        val_loss = eval_epoch(model, dataloaders['val'], loss_func)\n",
        "        val_time = round(time.time() - start_time)\n",
        "        \n",
        "        train_loss_hist.append(train_loss)\n",
        "        val_loss_hist.append(val_loss)\n",
        "        \n",
        "        print(f'Epoch {i} - train/val_time: {training_time}s | {val_time}s - train_loss: {train_loss:.4f} - val_loss: {val_loss:.4f}')\n",
        "        \n",
        "    return train_loss_hist, val_loss_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2vY_0FEADxd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4c4ec8e-c7a5-43f4-b53c-c4be4c7aab20"
      },
      "source": [
        "train_loss_hist, val_loss_hist = train_model(MySSD300, dataloaders, optimizer, exp_lr_scheduler, loss_func, epoch=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch 1 - step 0: train_loss: 2.9408\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "Epoch 1 - step 100: train_loss: 0.3721\n",
            "Epoch 1 - step 200: train_loss: 0.3657\n",
            "Epoch 1 - step 300: train_loss: 0.3943\n",
            "Epoch 1 - step 400: train_loss: 0.3530\n",
            "Epoch 1 - step 500: train_loss: 0.3488\n",
            "Epoch 1 - step 600: train_loss: 0.3269\n",
            "Epoch 1 - step 700: train_loss: 0.3676\n",
            "Epoch 1 - step 800: train_loss: 0.3516\n",
            "Epoch 1 - step 900: train_loss: 0.3488\n",
            "Epoch 1 - step 1000: train_loss: 0.3194\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 - train/val_time: 620s | 47s - train_loss: 0.3729 - val_loss: 0.0856\n",
            "Epoch 2 - train/val_time: 599s | 46s - train_loss: 0.3356 - val_loss: 0.0841\n",
            "Epoch 3 - Model saved at checkpoint_path | Time consumed: 1s\n",
            "Epoch 3 - train/val_time: 604s | 46s - train_loss: 0.3235 - val_loss: 0.0810\n",
            "Epoch 4 - train/val_time: 597s | 46s - train_loss: 0.3119 - val_loss: 0.0789\n",
            "Epoch 5 - train/val_time: 595s | 45s - train_loss: 0.3020 - val_loss: 0.0740\n",
            "Epoch 6 - Model saved at checkpoint_path | Time consumed: 1s\n",
            "Epoch 6 - train/val_time: 592s | 46s - train_loss: 0.2937 - val_loss: 0.0731\n",
            "Epoch 7 - train/val_time: 621s | 49s - train_loss: 0.2863 - val_loss: 0.0728\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch 8 - train/val_time: 657s | 49s - train_loss: 0.2809 - val_loss: 0.0723\n",
            "Epoch 9 - Model saved at checkpoint_path | Time consumed: 2s\n",
            "Epoch 9 - train/val_time: 658s | 50s - train_loss: 0.2760 - val_loss: 0.0706\n",
            "Epoch 10 - train/val_time: 654s | 50s - train_loss: 0.2715 - val_loss: 0.0672\n",
            "Epoch 11 - train/val_time: 661s | 49s - train_loss: 0.2697 - val_loss: 0.0675\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch 12 - Model saved at checkpoint_path | Time consumed: 2s\n",
            "Epoch 12 - train/val_time: 661s | 50s - train_loss: 0.2659 - val_loss: 0.0657\n",
            "Epoch 13 - train/val_time: 659s | 49s - train_loss: 0.2642 - val_loss: 0.0655\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch 14 - train/val_time: 657s | 49s - train_loss: 0.2607 - val_loss: 0.0651\n",
            "Epoch 15 - Model saved at checkpoint_path | Time consumed: 2s\n",
            "Epoch 15 - train/val_time: 658s | 50s - train_loss: 0.2593 - val_loss: 0.0639\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch 16 - train/val_time: 674s | 49s - train_loss: 0.2571 - val_loss: 0.0652\n",
            "Epoch 17 - train/val_time: 662s | 49s - train_loss: 0.2547 - val_loss: 0.0643\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch 18 - Model saved at checkpoint_path | Time consumed: 2s\n",
            "Epoch 18 - train/val_time: 662s | 50s - train_loss: 0.2530 - val_loss: 0.0634\n",
            "Epoch 19 - train/val_time: 634s | 46s - train_loss: 0.2503 - val_loss: 0.0641\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch 20 - train/val_time: 603s | 46s - train_loss: 0.2487 - val_loss: 0.0644\n",
            "Epoch 21 - Model saved at checkpoint_path | Time consumed: 1s\n",
            "Epoch 21 - train/val_time: 607s | 46s - train_loss: 0.2476 - val_loss: 0.0634\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch 22 - train/val_time: 598s | 45s - train_loss: 0.2462 - val_loss: 0.0621\n",
            "Epoch 23 - train/val_time: 600s | 46s - train_loss: 0.2449 - val_loss: 0.0613\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch 24 - Model saved at checkpoint_path | Time consumed: 1s\n",
            "Epoch 24 - train/val_time: 618s | 47s - train_loss: 0.2434 - val_loss: 0.0609\n",
            "Epoch 25 - train/val_time: 623s | 47s - train_loss: 0.2428 - val_loss: 0.0609\n",
            "Epoch 26 - train/val_time: 632s | 47s - train_loss: 0.2402 - val_loss: 0.0607\n",
            "Epoch 27 - Model saved at checkpoint_path | Time consumed: 1s\n",
            "Epoch 27 - train/val_time: 639s | 48s - train_loss: 0.2394 - val_loss: 0.0629\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Epoch 28 - train/val_time: 647s | 49s - train_loss: 0.2384 - val_loss: 0.0610\n",
            "Epoch 29 - train/val_time: 646s | 49s - train_loss: 0.2375 - val_loss: 0.0594\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Epoch 30 - Model saved at checkpoint_path | Time consumed: 2s\n",
            "Epoch 30 - train/val_time: 641s | 48s - train_loss: 0.2365 - val_loss: 0.0597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYUFDZmKMSZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLahWE9nMSZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSMYFtHaMSZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDrJxbWwMSZo",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VF5t9KrMSZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_epoch(model, valset_loader, loss_func):\n",
        "    '''\n",
        "    '''\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    \n",
        "    detected_boxes = list()\n",
        "    detected_labels = list()\n",
        "    detected_scores = list()\n",
        "    true_boxes = list()\n",
        "    true_labels = list()\n",
        "    true_diffs = list()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for step, (imgs, boxes, labels, diffs) in enumerate(valset_loader):\n",
        "            # move input data to GPU\n",
        "            imgs = imgs.to(device)\n",
        "            boxes = [b.to(device) for b in boxes]\n",
        "            labels = [l.to(device) for l in labels]\n",
        "            diffs = [d.to(device) for d in diffs]\n",
        "\n",
        "            # detect objects\n",
        "            predicted_offsets, predicted_scores = model(imgs)\n",
        "            batch_det_boxes, batch_det_labels, batch_det_scores = model.detect_objects(predicted_offsets, predicted_scores,\n",
        "                                                                                       score_threshold=0.01, iou_threshold=0.5)\n",
        "            # calculate val loss\n",
        "            loss = loss_func(predicted_offsets, predicted_scores, boxes, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            detected_boxes.extend(batch_det_boxes)\n",
        "            detected_labels.extend(batch_det_labels)\n",
        "            detected_scores.extend(batch_det_scores)\n",
        "            true_boxes.extend(boxes)\n",
        "            true_labels.extend(labels)\n",
        "            true_diffs.extend(diffs)\n",
        "\n",
        "        # calculate metrics value\n",
        "        APs, mAP = calculate_mAP(detected_boxes, detected_labels, detected_scores, true_boxes, true_labels, true_diffs)\n",
        "        val_loss = val_loss/len(valset)\n",
        "    \n",
        "    return val_loss, APs, mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TTeqqgQMSZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model_performance(model, valset_loader):\n",
        "    ''' This take into account all objects, including difficult ones\n",
        "    '''\n",
        "    model.eval()\n",
        "    detected_boxes = list()\n",
        "    detected_labels = list()\n",
        "    detected_scores = list()\n",
        "    true_boxes = list()\n",
        "    true_labels = list()\n",
        "    #true_diffs = list()\n",
        "    \n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for step, (imgs, boxes, labels, diffs) in enumerate(valset_loader):\n",
        "            # move input data to GPU\n",
        "            imgs = imgs.to(device)\n",
        "            boxes = [b.to(device) for b in boxes]\n",
        "            labels = [l.to(device) for l in labels]\n",
        "            #diffs = [d.to(device) for d in diffs]\n",
        "\n",
        "            # detect objects\n",
        "            predicted_offsets, predicted_scores = model(imgs)\n",
        "            batch_det_boxes, batch_det_labels, batch_det_scores = model.detect_objects(predicted_offsets, predicted_scores,\n",
        "                                                                                       score_threshold=0.01, iou_threshold=0.5)\n",
        "            detected_boxes.extend(batch_det_boxes)\n",
        "            detected_labels.extend(batch_det_labels)\n",
        "            detected_scores.extend(batch_det_scores)\n",
        "            true_boxes.extend(boxes)\n",
        "            true_labels.extend(labels)\n",
        "            #true_diffs.extend(diffs)\n",
        "\n",
        "        # calculate metrics value\n",
        "        APs, mAP = calculate_mAP(detected_boxes, detected_labels, detected_scores, true_boxes, true_labels)\n",
        "    \n",
        "    return APs, mAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpgOds2dMSZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "APs, mAP = eval_model_performance(MySSD300, dataloaders['val'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh50QZKc7lLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "dd53d551-03cc-4bc9-fbb4-9ae48f6fd544"
      },
      "source": [
        "APs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aeroplane': 0.3414236009120941,\n",
              " 'bicycle': 0.09090909361839294,\n",
              " 'bird': 0.09090909361839294,\n",
              " 'boat': 0.09090909361839294,\n",
              " 'bottle': 0.0,\n",
              " 'bus': 0.3882625997066498,\n",
              " 'car': 0.32834258675575256,\n",
              " 'cat': 0.29180702567100525,\n",
              " 'chair': 0.04545454680919647,\n",
              " 'cow': 0.04545454680919647,\n",
              " 'diningtable': 0.07070707529783249,\n",
              " 'dog': 0.24327227473258972,\n",
              " 'horse': 0.234884575009346,\n",
              " 'motorbike': 0.30315008759498596,\n",
              " 'person': 0.24644705653190613,\n",
              " 'pottedplant': 0.0,\n",
              " 'sheep': 0.09090909361839294,\n",
              " 'sofa': 0.09090909361839294,\n",
              " 'train': 0.14462809264659882,\n",
              " 'tvmonitor': 0.09090909361839294}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxWh2o1J7mR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6de518af-6254-4239-b64e-238715ab5e81"
      },
      "source": [
        "mAP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16146442294120789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CKH1BA5A_JP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}